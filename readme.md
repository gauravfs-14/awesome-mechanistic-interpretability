# Awesome Mechanistic Interpretability

![Awesome](https://awesome.re/badge.svg)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
![GitHub Contributors](https://img.shields.io/github/contributors/gauravfs-14/awesome-mechanistic-interpretability.svg)
![GitHub Last Commit](https://img.shields.io/github/last-commit/gauravfs-14/awesome-mechanistic-interpretability.svg)
[![GitHub Stars](https://img.shields.io/github/stars/gauravfs-14/awesome-mechanistic-interpretability.svg?style=social)](https://github.com/gauravfs-14/awesome-mechanistic-interpretability)
![GitHub Forks](https://img.shields.io/github/forks/gauravfs-14/awesome-mechanistic-interpretability.svg)

A carefully curated collection of high-quality libraries, projects, tutorials, research papers, and other essential resources focused on Mechanistic Interpretability, a growing subfield in machine learning interpretability research that aims to reverse-engineer neural networks into understandable computational components. This repository serves as a comprehensive and well-organized knowledge base for researchers, engineers, and enthusiasts working to uncover the inner workings of modern AI systems, particularly large language models (LLMs).

To ensure that the community stays updated on the latest developments, our repository is automatically updated with recent mechanistic interpretability papers from arXiv. This ensures timely access to new techniques, discoveries, and frameworks that are shaping the future of model transparency and alignment.

> [!NOTE]
> ðŸ“¢ **Announcement:** Our paper from AIT Lab is now available on [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5345552)!  
> **Title:** *Bridging the Black Box: A Survey on Mechanistic Interpretability in AI*  
> If you find this paper interesting, please consider citing our work. Thank you for your support!

```bibtex
@article{somvanshi2025bridging,
  title={Bridging the Black Box: A Survey on Mechanistic Interpretability in AI},
  author={Somvanshi, Shriyank and Islam, Md Monzurul and Rafe, Amir and Tusti, Anannya Ghosh and Chakraborty, Arka and Baitullah, Anika and Chowdhury, Tausif Islam and Alnawmasi, Nawaf and Dutta, Anandi and Das, Subasish},
  journal={Available at SSRN 5345552},
  year={2025}
}
```

Whether you are investigating the circuits behind in-context learning, decoding attention heads in transformers, or exploring interpretability tools like activation patching and causal tracing, this collection serves as a centralized hub for everything related to Mechanistic Interpretability â€” enriched by original peer-reviewed contributions and hands-on research from the broader interpretability community.

## Last Updated
November 2, 2025 at 01:14:12 AM UTC


## Theorem

## Papers (331)
- [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
- [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
- [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
- [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
- [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
- [RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs](https://arxiv.org/abs/2502.07424)
- [Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models](https://arxiv.org/abs/2412.14133)
- [A CRISP approach to QSP: XAI enabling fit-for-purpose models](https://arxiv.org/abs/2505.02750)
- [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
- [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
- [TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research](https://arxiv.org/abs/2503.12730)
- [Evaluating Neuron Explanations: A Unified Framework with Sanity Checks](https://arxiv.org/abs/2506.05774)
- [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613)
- [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)
- [The Vector Grounding Problem](https://arxiv.org/abs/2304.01481)
- [An analytic theory of creativity in convolutional diffusion models](https://arxiv.org/abs/2412.20292)
- [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
- [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)
- [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
- [Forecasting Seasonal Influenza Epidemics with Physics-Informed Neural Networks](https://arxiv.org/abs/2506.03897)
- [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
- [Tracking the Feature Dynamics in LLM Training: A Mechanistic Study](https://arxiv.org/abs/2412.17626)
- [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475)
- [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
- [Do Large Language Models (Really) Need Statistical Foundations?](https://arxiv.org/abs/2505.19145)
- [Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder](https://arxiv.org/abs/2506.02263)
- [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
- [Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions](https://arxiv.org/abs/2405.03205)
- [: Interpreting and leveraging semantic information in diffusion models](https://arxiv.org/abs/2411.16725)
- [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/abs/2505.24731)
- [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
- [Planning in a recurrent neural network that plays Sokoban](https://arxiv.org/abs/2407.15421)
- [Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability](https://arxiv.org/abs/2501.18887)
- [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
- [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319)
- [Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks](https://arxiv.org/abs/2410.03972)
- [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
- [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
- [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
- [In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention](https://arxiv.org/abs/2503.12734)
- [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
- [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
- [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
- [From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance](https://arxiv.org/abs/2505.20229)
- [Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks](https://arxiv.org/abs/2505.20132)
- [MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding](https://arxiv.org/abs/2505.15946)
- [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
- [DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces](https://arxiv.org/abs/2505.18441)
- [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
- [The Origins of Representation Manifolds in Large Language Models](https://arxiv.org/abs/2505.18235)
- [The Remarkable Robustness of LLMs: Stages of Inference?](https://arxiv.org/abs/2406.19384)
- [Monet: Mixture of Monosemantic Experts for Transformers](https://arxiv.org/abs/2412.04139)
- [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
- [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
- [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
- [Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis](https://arxiv.org/abs/2502.11812)
- [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
- [Sectoral Coupling in Linguistic State Space](https://arxiv.org/abs/2506.12927)
- [SAE-V: Interpreting Multimodal Models for Enhanced Alignment](https://arxiv.org/abs/2502.17514)
- [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
- [Interpretability and Generalization Bounds for Learning Spatial Physics](https://arxiv.org/abs/2506.15199)
- [A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning](https://arxiv.org/abs/2411.04105)
- [Mechanistic Interpretability in the Presence of Architectural Obfuscation](https://arxiv.org/abs/2506.18053)
- [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
- [Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference](https://arxiv.org/abs/2408.08590)
- [Validating Mechanistic Interpretations: An Axiomatic Approach](https://arxiv.org/abs/2407.13594)
- [Six Fallacies in Substituting Large Language Models for Human Participants](https://arxiv.org/abs/2402.04470)
- [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
- [From memories to maps: Mechanisms of in context reinforcement learning in transformers](https://arxiv.org/abs/2506.19686)
- [Measuring and Guiding Monosemanticity](https://arxiv.org/abs/2506.19382)
- [Emergent collective dynamics from motile photokinetic organisms](https://arxiv.org/abs/2506.19081)
- [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
- [Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition](https://arxiv.org/abs/2408.01139)
- [Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models](https://arxiv.org/abs/2410.01434)
- [Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers](https://arxiv.org/abs/2411.08745)
- [Bilinear MLPs enable weight-based mechanistic interpretability](https://arxiv.org/abs/2410.08417)
- [From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers](https://arxiv.org/abs/2506.19686)
- [Amortizing personalization in virtual brain twins](https://arxiv.org/abs/2506.21155)
- [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
- [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
- [Mechanistic Interpretability of Emotion Inference in Large Language Models](https://arxiv.org/abs/2502.05489)
- [Data-Driven Multiscale Topology Optimization of Spinodoid Architected Materials with Controllable Anisotropy](https://arxiv.org/abs/2506.23420)
- [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
- [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
- [Emerging AI Approaches for Cancer Spatial Omics](https://arxiv.org/abs/2506.23857)
- [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
- [Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery](https://arxiv.org/abs/2507.02730)
- [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
- [Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks](https://arxiv.org/abs/2502.06106)
- [Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment in LLMs](https://arxiv.org/abs/2507.03662)
- [Scientific Machine Learning of Chaotic Systems Discovers Governing Equations for Neural Populations](https://arxiv.org/abs/2507.03631)
- [Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding](https://arxiv.org/abs/2507.03197)
- [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
- [Dynamical Archetype Analysis: Autonomous Computation](https://arxiv.org/abs/2507.05505)
- [A statistical approach to latent dynamic modeling with differential equations](https://arxiv.org/abs/2311.16286)
- [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
- [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
- [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
- [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
- [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
- [A PBN-RL-XAI Framework for Discovering a "Hit-and-Run" Therapeutic Strategy in Melanoma](https://arxiv.org/abs/2507.10136)
- [Propensity score weighting across counterfactual worlds: longitudinal effects under positivity violations](https://arxiv.org/abs/2507.10774)
- [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
- [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
- [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
- [FADE: Why Bad Descriptions Happen to Good Features](https://arxiv.org/abs/2502.16994)
- [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
- [Teach Old SAEs New Domain Tricks with Boosting](https://arxiv.org/abs/2507.12990)
- [Insights into a radiology-specialised multimodal large language model with sparse autoencoders](https://arxiv.org/abs/2507.12950)
- [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
- [Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models](https://arxiv.org/abs/2412.16247)
- [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
- [Decoding Translation-Related Functional Sequences in 5'UTRs Using Interpretable Deep Learning Models](https://arxiv.org/abs/2507.16801)
- [Probing Ranking LLMs: A Mechanistic Analysis for Information Retrieval](https://arxiv.org/abs/2410.18527)
- [Deep Learning for Blood-Brain Barrier Permeability Prediction](https://arxiv.org/abs/2507.18557)
- [Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input](https://arxiv.org/abs/2507.18396)
- [CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing](https://arxiv.org/abs/2507.19420)
- [HumorDB: Can AI understand graphical humor?](https://arxiv.org/abs/2406.13564)
- [Analyze Feature Flow to Enhance Interpretation and Steering in Language Models](https://arxiv.org/abs/2502.03032)
- [A nonparametric approach to practical identifiability of nonlinear mixed effects models](https://arxiv.org/abs/2507.20288)
- [Large Language Models Are Human-Like Internally](https://arxiv.org/abs/2502.01615)
- [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
- [Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses](https://arxiv.org/abs/2507.21132)
- [Semiclassical Spin Exchange via Temperature-Dependent Transition States](https://arxiv.org/abs/2507.22700)
- [Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders](https://arxiv.org/abs/2507.23220)
- [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)
- [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)
- [Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning](https://arxiv.org/abs/2508.01916)
- [Modeling the Temperature-Humidity Coupling Dynamics of Soybean Pod Borer Population and Assessing the Predictive Performance of the PCM-NN Algorithm](https://arxiv.org/abs/2508.03238)
- [I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878)
- [Covariance spectrum in nonlinear recurrent neural networks](https://arxiv.org/abs/2508.05288)
- [Discovery of Disease Relationships via Transcriptomic Signature Analysis Powered by Agentic AI](https://arxiv.org/abs/2508.04742)
- [Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality](https://arxiv.org/abs/2503.24277)
- [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with (IA^3IA^3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
- [Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs](https://arxiv.org/abs/2508.09019)
- [Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models](https://arxiv.org/abs/2508.08879)
- [BiasGym: Fantastic Biases and How to Find (and Remove) Them](https://arxiv.org/abs/2508.08855)
- [From Transformer to Biology: A Hierarchical Model for Attention in Complex Problem-Solving](https://arxiv.org/abs/2406.14100)
- [How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence](https://arxiv.org/abs/2504.02904)
- [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
- [BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them](https://arxiv.org/abs/2508.08855)
- [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
- [Maximum Entropy Models for Unimodal Time Series: Case Studies of Universe 25 and St. Matthew Island](https://arxiv.org/abs/2508.10518)
- [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
- [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
- [Counterfactual Probabilistic Diffusion with Expert Models](https://arxiv.org/abs/2508.13355)
- [Modeling GRNs with a Probabilistic Categorical Framework](https://arxiv.org/abs/2508.13208)
- [Disentangling concept semantics via multilingual averaging in Sparse Autoencoders](https://arxiv.org/abs/2508.14275)
- [LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts](https://arxiv.org/abs/2508.16325)
- [From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits](https://arxiv.org/abs/2508.16109)
- [Fidelity Isn't Accuracy: When Linearly Decodable Functions Fail to Match the Ground Truth](https://arxiv.org/abs/2506.12176)
- [Beyond Transcription: Mechanistic Interpretability in ASR](https://arxiv.org/abs/2508.15882)
- [Mechanistic Exploration of Backdoored Large Language Model Attention Patterns](https://arxiv.org/abs/2508.15847)
- [KL-based self-distillation for large language models](https://arxiv.org/abs/2508.15807)
- [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)
- [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
- [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions](https://arxiv.org/abs/2508.16950)
- [Rethinking scale in network neuroscience: Contributions and opportunities at the nanoscale](https://arxiv.org/abs/2508.16760)
- [Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability](https://arxiv.org/abs/2508.16652)
- [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
- [Tracing Positional Bias in Financial Decision-Making: Mechanistic Insights from Qwen2.5](https://arxiv.org/abs/2508.18427)
- [Linear Power System Modeling and Analysis Across Wide Operating Ranges: A Hierarchical Neural State-Space Equation Approach](https://arxiv.org/abs/2508.17774)
- [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
- [RelP: Faithful and Efficient Circuit Discovery via Relevance Patching](https://arxiv.org/abs/2508.21258)
- [Prompt the Unseen: Evaluating Visual-Language Alignment Beyond Supervision](https://arxiv.org/abs/2509.00700)
- [Mechanistic interpretability for steering vision-language-action models](https://arxiv.org/abs/2509.00328)
- [Can LLMs Lie? Investigation beyond Hallucination](https://arxiv.org/abs/2509.03518)
- [Challenges in Understanding Modality Conflict in Vision-Language Models](https://arxiv.org/abs/2509.02805)
- [Non-Linear Model-Based Sequential Decision-Making in Agriculture](https://arxiv.org/abs/2509.01924)
- [Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function](https://arxiv.org/abs/2509.01874)
- [Pulling Back the Curtain on ReLU Networks](https://arxiv.org/abs/2507.22832)
- [Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces](https://arxiv.org/abs/2509.03738)
- [Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects](https://arxiv.org/abs/2509.04794)
- [Interpreting Transformer Architectures as Implicit Multinomial Regression](https://arxiv.org/abs/2509.04653)
- [ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders](https://arxiv.org/abs/2509.05309)
- [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
- [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
- [Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning](https://arxiv.org/abs/2412.01113)
- [Data-driven discovery of dynamical models in biology](https://arxiv.org/abs/2509.06735)
- [Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors](https://arxiv.org/abs/2509.06608)
- [The Quest for the Right Mediator: Surveying Mechanistic Interpretability Through the Lens of Causal Mediation Analysis](https://arxiv.org/abs/2408.01416)
- [Interpretability as Alignment: Making Internal Understanding a Design Principle](https://arxiv.org/abs/2509.08592)
- [Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition](https://arxiv.org/abs/2509.08454)
- [Decoding the Stability of Transition-Metal Alloys with Theory-infused Deep Learning](https://arxiv.org/abs/2506.03031)
- [An Agentic AI Workflow to Simplify Parameter Estimation of Complex Differential Equation Systems](https://arxiv.org/abs/2509.07283)
- [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
- [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
- [Modelling Under-Reported Data: Pitfalls of NaÃ¯ve Approaches and a New Statistical Framework for Epidemic Curve Reconstruction](https://arxiv.org/abs/2509.10668)
- [The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features](https://arxiv.org/abs/2509.12934)
- [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)
- [Swarm Intelligence for Chemical Reaction Optimisation](https://arxiv.org/abs/2509.11798)
- [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
- [Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model](https://arxiv.org/abs/2509.15124)
- [Unified Spatiotemporal Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
- [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
- [DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction](https://arxiv.org/abs/2509.15872)
- [Modeling Transformers as complex networks to analyze learning dynamics](https://arxiv.org/abs/2509.15269)
- [Bayesian Calibration and Model Assessment of Cell Migration Dynamics with Surrogate Model Integration](https://arxiv.org/abs/2509.18998)
- [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
- [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
- [Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models](https://arxiv.org/abs/2509.17665)
- [Tikhonov-Fenichel Reductions and their Application to a Novel Modelling Approach for Mutualism](https://arxiv.org/abs/2501.12069)
- [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
- [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
- [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
- [BioBO: Biology-informed Bayesian Optimization for Perturbation Design](https://arxiv.org/abs/2509.19988)
- [Interpreting ResNet-based CLIP via Neuron-Attention Decomposition](https://arxiv.org/abs/2509.19943)
- [Integrating Mechanistic Modeling and Machine Learning to Study CD4+/CD8+ CAR-T Cell Dynamics with Tumor Antigen Regulation](https://arxiv.org/abs/2509.19536)
- [Fine-Tuning is Subgraph Search: A New Lens on Learning Dynamics](https://arxiv.org/abs/2502.06106)
- [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
- [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
- [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
- [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
- [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
- [RAPTOR-GEN: RApid PosTeriOR GENerator for Bayesian Learning in Biomanufacturing](https://arxiv.org/abs/2509.20753)
- [Concept-SAE: Active Causal Probing of Visual Model Behavior](https://arxiv.org/abs/2509.22015)
- [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
- [Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](https://arxiv.org/abs/2509.21761)
- [Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare](https://arxiv.org/abs/2502.13319)
- [Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs](https://arxiv.org/abs/2509.23684)
- [LLM Interpretability with Identifiable Temporal-Instantaneous Representation](https://arxiv.org/abs/2509.23323)
- [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
- [Bayesian Inference for Sexual Contact Networks Using Longitudinal Survey Data](https://arxiv.org/abs/2509.22848)
- [Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://arxiv.org/abs/2509.22831)
- [Thrust based on changes in angular momentum](https://arxiv.org/abs/2105.10775)
- [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
- [ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack](https://arxiv.org/abs/2509.25843)
- [Minimalist Explanation Generation and Circuit Discovery](https://arxiv.org/abs/2509.25686)
- [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
- [Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF](https://arxiv.org/abs/2509.24713)
- [Excitonic Energy Transfer in Red Algal Photosystem I Reveals an Evolutionary Bridge between Cyanobacteria and Plants](https://arxiv.org/abs/2509.24271)
- [Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis](https://arxiv.org/abs/2509.24164)
- [From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning](https://arxiv.org/abs/2509.23768)
- [Measuring Sparse Autoencoder Feature Sensitivity](https://arxiv.org/abs/2509.23717)
- [Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI](https://arxiv.org/abs/2509.25220)
- [Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG](https://arxiv.org/abs/2510.00845)
- [Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models](https://arxiv.org/abs/2503.17349)
- [Feature Identification via the Empirical NTK](https://arxiv.org/abs/2510.00468)
- [Commutative algebra neural network reveals genetic origins of diseases](https://arxiv.org/abs/2509.26566)
- [Interpret, prune and distill Donut : towards lightweight VLMs for VQA on document](https://arxiv.org/abs/2509.26235)
- [BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning](https://arxiv.org/abs/2510.01632)
- [When Reasoning Meets Compression: Understanding the Effects of LLMs Compression on Large Reasoning Models](https://arxiv.org/abs/2504.02010)
- [Integrative modelling of biomolecular dynamics](https://arxiv.org/abs/2510.01108)
- [Interpreting Language Models Through Concept Descriptions: A Survey](https://arxiv.org/abs/2510.01048)
- [Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification](https://arxiv.org/abs/2504.17017)
- [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
- [Understanding Addition and Subtraction in Transformers](https://arxiv.org/abs/2402.02619)
- [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
- [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
- [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
- [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
- [Deep learning for flash drought forecasting and interpretation](https://arxiv.org/abs/2510.02576)
- [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
- [Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
- [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
- [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
- [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
- [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
- [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
- [Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models](https://arxiv.org/abs/2510.06107)
- [Mechanistic-statistical inference of mosquito dynamics from mark-release-recapture data](https://arxiv.org/abs/2510.06080)
- [Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?](https://arxiv.org/abs/2510.06036)
- [The Logical Implication Steering Method for Conditional Interventions on Transformer Generation](https://arxiv.org/abs/2502.03618)
- [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
- [Causal Abstractions, Categorically Unified](https://arxiv.org/abs/2510.05033)
- [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
- [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
- [Tug-of-war between idioms' figurative and literal interpretations in LLMs](https://arxiv.org/abs/2506.01723)
- [ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall](https://arxiv.org/abs/2510.07896)
- [Advancing AI Research Assistants with Expert-Involved Learning](https://arxiv.org/abs/2505.04638)
- [Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin](https://arxiv.org/abs/2510.08407)
- [Iterated Agent for Symbolic Regression](https://arxiv.org/abs/2510.08317)
- [RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation](https://arxiv.org/abs/2510.08931)
- [InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques](https://arxiv.org/abs/2407.14494)
- [Egocentric Visual Navigation through Hippocampal Sequences](https://arxiv.org/abs/2510.09951)
- [Causality \neq\neq Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs](https://arxiv.org/abs/2510.09794)
- [Impact of Oxygen on DNA Damage Distribution in 3D Genome and Its Correlation to Oxygen Enhancement Ratio under High LET Irradiation](https://arxiv.org/abs/2503.21837)
- [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
- [QLENS: Towards A Quantum Perspective of Language Transformers](https://arxiv.org/abs/2510.11963)
- [A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models](https://arxiv.org/abs/2407.02646)
- [Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas](https://arxiv.org/abs/2503.01773)
- [Data-Driven Topology Optimization for Multiscale Biomimetic Spinodal Design](https://arxiv.org/abs/2506.23420)
- [Physical models of embryonic epithelial healing: A review](https://arxiv.org/abs/2510.13374)
- [Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs](https://arxiv.org/abs/2510.13251)
- [Multi-Scale Probabilistic Generation Theory: A Unified Information-Theoretic Framework for Hierarchical Structure in Large Language Models](https://arxiv.org/abs/2505.18244)
- [Constrained belief updates explain geometric structures in transformer representations](https://arxiv.org/abs/2502.01954)
- [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
- [Functional and parametric identifiability for universal differential equations applied to chemical reaction networks](https://arxiv.org/abs/2510.14140)
- [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
- [Circuit Insights: Towards Interpretability Beyond Activations](https://arxiv.org/abs/2510.14936)
- [Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift](https://arxiv.org/abs/2510.15265)
- [Game-Theoretic Discovery of Quantum Error-Correcting Codes Through Nash Equilibria](https://arxiv.org/abs/2510.15223)
- [Intrinsic Self-Correction in LLMs: Towards Explainable Prompting via Mechanistic Interpretability](https://arxiv.org/abs/2505.11924)
- [DePass: Unified Feature Attributing by Simple Decomposed Forward Pass](https://arxiv.org/abs/2510.18462)
- [Extracting Rule-based Descriptions of Attention Features in Transformers](https://arxiv.org/abs/2510.18148)
- [How role-play shapes relevance judgment in zero-shot LLM rankers](https://arxiv.org/abs/2510.17535)
- [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
- [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
- [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
- [Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models](https://arxiv.org/abs/2510.17909)
- [Base Models Know How to Reason, Thinking Models Learn When](https://arxiv.org/abs/2510.07364)
- [I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs](https://arxiv.org/abs/2510.19678)
- [A Class of Markovian Self-Reinforcing Processes with Power-Law Distributions](https://arxiv.org/abs/2510.19034)
- [Prospects for Using Artificial Intelligence to Understand Intrinsic Kinetics of Heterogeneous Catalytic Reactions](https://arxiv.org/abs/2510.18911)
- [Foundation Models for Discovery and Exploration in Chemical Space](https://arxiv.org/abs/2510.18900)
- [Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons](https://arxiv.org/abs/2406.14144)
- [MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction](https://arxiv.org/abs/2510.20448)
- [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
- [Some Attention is All You Need for Retrieval](https://arxiv.org/abs/2510.19861)
- [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/abs/2505.13737)
- [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)
- [Mapping Faithful Reasoning in Language Models](https://arxiv.org/abs/2510.22362)
- [Transformer brain encoders explain human high-level visual responses](https://arxiv.org/abs/2505.17329)
- [Mechanistic Interpretability for Neural TSP Solvers](https://arxiv.org/abs/2510.21693)
- [Mechanism-Guided Residual Lifting and Control Consistent Modeling for Pneumatic Drying Processes](https://arxiv.org/abs/2510.24370)
- [Overshoot-resolved transition modeling based on field inversion and symbolic regression](https://arxiv.org/abs/2510.24192)
- [Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders](https://arxiv.org/abs/2510.23802)
- [PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization](https://arxiv.org/abs/2510.23264)
- [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
- [Sparsity and Superposition in Mixture of Experts](https://arxiv.org/abs/2510.23671)
- [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
- [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
- [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
- [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
- [RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching](https://arxiv.org/abs/2508.21258)
- [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
- [In Defence of Post-hoc Explainability](https://arxiv.org/abs/2412.17883)
- [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
- [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
- [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
- [Modelling ion channels with a view towards identifiability](https://arxiv.org/abs/2510.26728)


### Dedicated Publication Threads
- [Anthropicâ€™s Interpretability Research](https://transformer-circuits.pub)
- [Distill: Circuits Thread](https://distill.pub/2020/circuits/)

## Library

## Tutorial

### Written Tutorials
- [Mechanistic Interpretability for LLMs, explained](https://seantrott.substack.com/p/mechanistic-interpretability-for) by [The CounterFactual](https://seantrott.substack.com)

### Video Tutorials

## Contributing

We welcome contributions to this repository! If you have a resource that you believe should be included, please submit a pull request or open an issue. Contributions can include:

- New libraries or tools related to mechanistic interpretability
- Tutorials or guides that help users understand and implement mechanistic interpretability techniques
- Research papers that advance the field of mechanistic interpretability
- Any other resources that you find valuable for the community

## How to Contribute

1. Fork the repository.
2. Create a new branch for your changes.
3. Make your changes and commit them with a clear message.
4. Push your changes to your forked repository.
5. Submit a pull request to the main repository.

Before contributing, take a look at the existing resources to avoid duplicates.

## License

This repository is licensed under the [Creative Commons Attribution 4.0 International License (CC BY 4.0)](LICENSE). You are free to share and adapt the material, provided you give appropriate credit, link to the license, and indicate if changes were made.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=gauravfs-14/awesome-mechanistic-interpretability)](https://star-history.com/#gauravfs-14/awesome-mechanistic-interpretability&Date)
