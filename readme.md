# Awesome Mechanistic Interpretability

![Awesome](https://awesome.re/badge.svg)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
![GitHub Contributors](https://img.shields.io/github/contributors/gauravfs-14/awesome-mechanistic-interpretability.svg)
![GitHub Last Commit](https://img.shields.io/github/last-commit/gauravfs-14/awesome-mechanistic-interpretability.svg)
[![GitHub Stars](https://img.shields.io/github/stars/gauravfs-14/awesome-mechanistic-interpretability.svg?style=social)](https://github.com/gauravfs-14/awesome-mechanistic-interpretability)
![GitHub Forks](https://img.shields.io/github/forks/gauravfs-14/awesome-mechanistic-interpretability.svg)

A carefully curated collection of high-quality libraries, projects, tutorials, research papers, and other essential resources focused on Mechanistic Interpretability, a growing subfield in machine learning interpretability research that aims to reverse-engineer neural networks into understandable computational components. This repository serves as a comprehensive and well-organized knowledge base for researchers, engineers, and enthusiasts working to uncover the inner workings of modern AI systems, particularly large language models (LLMs).

To ensure that the community stays updated on the latest developments, our repository is automatically updated with recent mechanistic interpretability papers from arXiv. This ensures timely access to new techniques, discoveries, and frameworks that are shaping the future of model transparency and alignment.

Whether you are investigating the circuits behind in-context learning, decoding attention heads in transformers, or exploring interpretability tools like activation patching and causal tracing, this collection serves as a centralized hub for everything related to Mechanistic Interpretability â€” enriched by original peer-reviewed contributions and hands-on research from the broader interpretability community.

## Last Updated
June 22, 2025 at 01:20:15 AM UTC


## Theorem

## Papers (61)
- [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
- [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
- [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
- [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
- [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
- [RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs](https://arxiv.org/abs/2502.07424)
- [Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models](https://arxiv.org/abs/2412.14133)
- [A CRISP approach to QSP: XAI enabling fit-for-purpose models](https://arxiv.org/abs/2505.02750)
- [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
- [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
- [TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research](https://arxiv.org/abs/2503.12730)
- [Evaluating Neuron Explanations: A Unified Framework with Sanity Checks](https://arxiv.org/abs/2506.05774)
- [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613)
- [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)
- [The Vector Grounding Problem](https://arxiv.org/abs/2304.01481)
- [An analytic theory of creativity in convolutional diffusion models](https://arxiv.org/abs/2412.20292)
- [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
- [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)
- [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
- [Forecasting Seasonal Influenza Epidemics with Physics-Informed Neural Networks](https://arxiv.org/abs/2506.03897)
- [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
- [Tracking the Feature Dynamics in LLM Training: A Mechanistic Study](https://arxiv.org/abs/2412.17626)
- [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475)
- [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
- [Do Large Language Models (Really) Need Statistical Foundations?](https://arxiv.org/abs/2505.19145)
- [Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder](https://arxiv.org/abs/2506.02263)
- [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
- [Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions](https://arxiv.org/abs/2405.03205)
- [: Interpreting and leveraging semantic information in diffusion models](https://arxiv.org/abs/2411.16725)
- [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/abs/2505.24731)
- [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
- [Planning in a recurrent neural network that plays Sokoban](https://arxiv.org/abs/2407.15421)
- [Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability](https://arxiv.org/abs/2501.18887)
- [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
- [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319)
- [Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks](https://arxiv.org/abs/2410.03972)
- [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
- [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
- [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
- [In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention](https://arxiv.org/abs/2503.12734)
- [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
- [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
- [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
- [From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance](https://arxiv.org/abs/2505.20229)
- [Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks](https://arxiv.org/abs/2505.20132)
- [MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding](https://arxiv.org/abs/2505.15946)
- [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
- [DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces](https://arxiv.org/abs/2505.18441)
- [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
- [The Origins of Representation Manifolds in Large Language Models](https://arxiv.org/abs/2505.18235)
- [The Remarkable Robustness of LLMs: Stages of Inference?](https://arxiv.org/abs/2406.19384)
- [Monet: Mixture of Monosemantic Experts for Transformers](https://arxiv.org/abs/2412.04139)
- [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
- [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
- [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
- [Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis](https://arxiv.org/abs/2502.11812)
- [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
- [Sectoral Coupling in Linguistic State Space](https://arxiv.org/abs/2506.12927)
- [SAE-V: Interpreting Multimodal Models for Enhanced Alignment](https://arxiv.org/abs/2502.17514)
- [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
- [Interpretability and Generalization Bounds for Learning Spatial Physics](https://arxiv.org/abs/2506.15199)


## Library

## Tutorial

### Written Tutorials

### Video Tutorials

## Contributing

We welcome contributions to this repository! If you have a resource that you believe should be included, please submit a pull request or open an issue. Contributions can include:

- New libraries or tools related to mechanistic interpretability
- Tutorials or guides that help users understand and implement mechanistic interpretability techniques
- Research papers that advance the field of mechanistic interpretability
- Any other resources that you find valuable for the community

## How to Contribute

1. Fork the repository.
2. Create a new branch for your changes.
3. Make your changes and commit them with a clear message.
4. Push your changes to your forked repository.
5. Submit a pull request to the main repository.

Before contributing, take a look at the existing resources to avoid duplicates.

## License

This repository is licensed under the [Creative Commons Attribution 4.0 International License (CC BY 4.0)](LICENSE). You are free to share and adapt the material, provided you give appropriate credit, link to the license, and indicate if changes were made.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=gauravfs-14/awesome-mechanistic-interpretability)](https://star-history.com/#gauravfs-14/awesome-mechanistic-interpretability&Date)
