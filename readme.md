# Awesome Mechanistic Interpretability

![Awesome](https://awesome.re/badge.svg)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
![GitHub Contributors](https://img.shields.io/github/contributors/gauravfs-14/awesome-mechanistic-interpretability.svg)
![GitHub Last Commit](https://img.shields.io/github/last-commit/gauravfs-14/awesome-mechanistic-interpretability.svg)
[![GitHub Stars](https://img.shields.io/github/stars/gauravfs-14/awesome-mechanistic-interpretability.svg?style=social)](https://github.com/gauravfs-14/awesome-mechanistic-interpretability)
![GitHub Forks](https://img.shields.io/github/forks/gauravfs-14/awesome-mechanistic-interpretability.svg)

A carefully curated collection of high-quality libraries, projects, tutorials, research papers, and other essential resources focused on Mechanistic Interpretability, a growing subfield in machine learning interpretability research that aims to reverse-engineer neural networks into understandable computational components. This repository serves as a comprehensive and well-organized knowledge base for researchers, engineers, and enthusiasts working to uncover the inner workings of modern AI systems, particularly large language models (LLMs).

To ensure that the community stays updated on the latest developments, our repository is automatically updated with recent mechanistic interpretability papers from arXiv. This ensures timely access to new techniques, discoveries, and frameworks that are shaping the future of model transparency and alignment.

Whether you are investigating the circuits behind in-context learning, decoding attention heads in transformers, or exploring interpretability tools like activation patching and causal tracing, this collection serves as a centralized hub for everything related to Mechanistic Interpretability â€” enriched by original peer-reviewed contributions and hands-on research from the broader interpretability community.

## Last Updated
July 31, 2025 at 01:19:00 AM UTC


## Theorem

## Papers (124)
- [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
- [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
- [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
- [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
- [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
- [RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs](https://arxiv.org/abs/2502.07424)
- [Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models](https://arxiv.org/abs/2412.14133)
- [A CRISP approach to QSP: XAI enabling fit-for-purpose models](https://arxiv.org/abs/2505.02750)
- [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
- [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
- [TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research](https://arxiv.org/abs/2503.12730)
- [Evaluating Neuron Explanations: A Unified Framework with Sanity Checks](https://arxiv.org/abs/2506.05774)
- [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613)
- [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)
- [The Vector Grounding Problem](https://arxiv.org/abs/2304.01481)
- [An analytic theory of creativity in convolutional diffusion models](https://arxiv.org/abs/2412.20292)
- [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
- [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)
- [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
- [Forecasting Seasonal Influenza Epidemics with Physics-Informed Neural Networks](https://arxiv.org/abs/2506.03897)
- [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
- [Tracking the Feature Dynamics in LLM Training: A Mechanistic Study](https://arxiv.org/abs/2412.17626)
- [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475)
- [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
- [Do Large Language Models (Really) Need Statistical Foundations?](https://arxiv.org/abs/2505.19145)
- [Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder](https://arxiv.org/abs/2506.02263)
- [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
- [Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions](https://arxiv.org/abs/2405.03205)
- [: Interpreting and leveraging semantic information in diffusion models](https://arxiv.org/abs/2411.16725)
- [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/abs/2505.24731)
- [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
- [Planning in a recurrent neural network that plays Sokoban](https://arxiv.org/abs/2407.15421)
- [Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability](https://arxiv.org/abs/2501.18887)
- [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
- [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319)
- [Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks](https://arxiv.org/abs/2410.03972)
- [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
- [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
- [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
- [In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention](https://arxiv.org/abs/2503.12734)
- [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
- [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
- [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
- [From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance](https://arxiv.org/abs/2505.20229)
- [Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks](https://arxiv.org/abs/2505.20132)
- [MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding](https://arxiv.org/abs/2505.15946)
- [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
- [DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces](https://arxiv.org/abs/2505.18441)
- [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
- [The Origins of Representation Manifolds in Large Language Models](https://arxiv.org/abs/2505.18235)
- [The Remarkable Robustness of LLMs: Stages of Inference?](https://arxiv.org/abs/2406.19384)
- [Monet: Mixture of Monosemantic Experts for Transformers](https://arxiv.org/abs/2412.04139)
- [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
- [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
- [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
- [Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis](https://arxiv.org/abs/2502.11812)
- [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
- [Sectoral Coupling in Linguistic State Space](https://arxiv.org/abs/2506.12927)
- [SAE-V: Interpreting Multimodal Models for Enhanced Alignment](https://arxiv.org/abs/2502.17514)
- [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
- [Interpretability and Generalization Bounds for Learning Spatial Physics](https://arxiv.org/abs/2506.15199)
- [A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning](https://arxiv.org/abs/2411.04105)
- [Mechanistic Interpretability in the Presence of Architectural Obfuscation](https://arxiv.org/abs/2506.18053)
- [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
- [Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference](https://arxiv.org/abs/2408.08590)
- [Validating Mechanistic Interpretations: An Axiomatic Approach](https://arxiv.org/abs/2407.13594)
- [Six Fallacies in Substituting Large Language Models for Human Participants](https://arxiv.org/abs/2402.04470)
- [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
- [From memories to maps: Mechanisms of in context reinforcement learning in transformers](https://arxiv.org/abs/2506.19686)
- [Measuring and Guiding Monosemanticity](https://arxiv.org/abs/2506.19382)
- [Emergent collective dynamics from motile photokinetic organisms](https://arxiv.org/abs/2506.19081)
- [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
- [Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition](https://arxiv.org/abs/2408.01139)
- [Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models](https://arxiv.org/abs/2410.01434)
- [Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers](https://arxiv.org/abs/2411.08745)
- [Bilinear MLPs enable weight-based mechanistic interpretability](https://arxiv.org/abs/2410.08417)
- [From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers](https://arxiv.org/abs/2506.19686)
- [Amortizing personalization in virtual brain twins](https://arxiv.org/abs/2506.21155)
- [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
- [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
- [Mechanistic Interpretability of Emotion Inference in Large Language Models](https://arxiv.org/abs/2502.05489)
- [Data-Driven Multiscale Topology Optimization of Spinodoid Architected Materials with Controllable Anisotropy](https://arxiv.org/abs/2506.23420)
- [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
- [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
- [Emerging AI Approaches for Cancer Spatial Omics](https://arxiv.org/abs/2506.23857)
- [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
- [Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery](https://arxiv.org/abs/2507.02730)
- [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
- [Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks](https://arxiv.org/abs/2502.06106)
- [Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment in LLMs](https://arxiv.org/abs/2507.03662)
- [Scientific Machine Learning of Chaotic Systems Discovers Governing Equations for Neural Populations](https://arxiv.org/abs/2507.03631)
- [Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding](https://arxiv.org/abs/2507.03197)
- [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
- [Dynamical Archetype Analysis: Autonomous Computation](https://arxiv.org/abs/2507.05505)
- [A statistical approach to latent dynamic modeling with differential equations](https://arxiv.org/abs/2311.16286)
- [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
- [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
- [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
- [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
- [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
- [A PBN-RL-XAI Framework for Discovering a "Hit-and-Run" Therapeutic Strategy in Melanoma](https://arxiv.org/abs/2507.10136)
- [Propensity score weighting across counterfactual worlds: longitudinal effects under positivity violations](https://arxiv.org/abs/2507.10774)
- [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
- [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
- [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
- [FADE: Why Bad Descriptions Happen to Good Features](https://arxiv.org/abs/2502.16994)
- [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
- [Teach Old SAEs New Domain Tricks with Boosting](https://arxiv.org/abs/2507.12990)
- [Insights into a radiology-specialised multimodal large language model with sparse autoencoders](https://arxiv.org/abs/2507.12950)
- [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
- [Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models](https://arxiv.org/abs/2412.16247)
- [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
- [Decoding Translation-Related Functional Sequences in 5'UTRs Using Interpretable Deep Learning Models](https://arxiv.org/abs/2507.16801)
- [Probing Ranking LLMs: A Mechanistic Analysis for Information Retrieval](https://arxiv.org/abs/2410.18527)
- [Deep Learning for Blood-Brain Barrier Permeability Prediction](https://arxiv.org/abs/2507.18557)
- [Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input](https://arxiv.org/abs/2507.18396)
- [CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing](https://arxiv.org/abs/2507.19420)
- [HumorDB: Can AI understand graphical humor?](https://arxiv.org/abs/2406.13564)
- [Analyze Feature Flow to Enhance Interpretation and Steering in Language Models](https://arxiv.org/abs/2502.03032)
- [A nonparametric approach to practical identifiability of nonlinear mixed effects models](https://arxiv.org/abs/2507.20288)
- [Large Language Models Are Human-Like Internally](https://arxiv.org/abs/2502.01615)
- [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
- [Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses](https://arxiv.org/abs/2507.21132)
- [Semiclassical Spin Exchange via Temperature-Dependent Transition States](https://arxiv.org/abs/2507.22700)


## Library

## Tutorial

### Written Tutorials

### Video Tutorials

## Contributing

We welcome contributions to this repository! If you have a resource that you believe should be included, please submit a pull request or open an issue. Contributions can include:

- New libraries or tools related to mechanistic interpretability
- Tutorials or guides that help users understand and implement mechanistic interpretability techniques
- Research papers that advance the field of mechanistic interpretability
- Any other resources that you find valuable for the community

## How to Contribute

1. Fork the repository.
2. Create a new branch for your changes.
3. Make your changes and commit them with a clear message.
4. Push your changes to your forked repository.
5. Submit a pull request to the main repository.

Before contributing, take a look at the existing resources to avoid duplicates.

## License

This repository is licensed under the [Creative Commons Attribution 4.0 International License (CC BY 4.0)](LICENSE). You are free to share and adapt the material, provided you give appropriate credit, link to the license, and indicate if changes were made.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=gauravfs-14/awesome-mechanistic-interpretability)](https://star-history.com/#gauravfs-14/awesome-mechanistic-interpretability&Date)
